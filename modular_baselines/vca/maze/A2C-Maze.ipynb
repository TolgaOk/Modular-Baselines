{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Example Pong-ram training using A2C. \"\"\"\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from modular_baselines.buffers.buffer import RolloutBuffer\n",
    "from modular_baselines.collectors.collector import OnPolicyCollector\n",
    "from modular_baselines.algorithms.a2c import A2C\n",
    "from modular_baselines.loggers.basic import(InitLogCallback,\n",
    "                                            LogRolloutCallback,\n",
    "                                            LogWeightCallback,\n",
    "                                            LogGradCallback)\n",
    "from modular_baselines.utils.wrappers import (NormalizeObservation,\n",
    "                                              SkipSteps,\n",
    "                                              AggregateObservation,\n",
    "                                              IndexObsevation,\n",
    "                                              IndexAction,\n",
    "                                              ResetWithNonZeroReward)\n",
    "from modular_baselines.vca.maze.environment import MazeEnv\n",
    "\n",
    "\n",
    "class Onehotter(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(self.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1,\n",
    "                                     shape=(env.observation_space.n,),\n",
    "                                     dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        n_states = self.observation_space.shape[0]\n",
    "        return (np.arange(n_states) == observation).astype(np.float32)\n",
    "\n",
    "\n",
    "def wrap_env():\n",
    "    return Onehotter(MazeEnv())\n",
    "\n",
    "\n",
    "class Policy(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, observation_space, action_space, hidden_size=16, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if not isinstance(observation_space, gym.spaces.Box):\n",
    "            raise ValueError(\"Unsupported observation space {}\".format(\n",
    "                observation_space))\n",
    "        if not isinstance(action_space, gym.spaces.Discrete):\n",
    "            raise ValueError(\"Unsupported action space {}\".format(\n",
    "                observation_space))\n",
    "\n",
    "        self.action_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(observation_space.shape[0], hidden_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_size, action_space.n)\n",
    "        )\n",
    "        self.value_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(observation_space.shape[0], hidden_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.RMSprop(self.parameters(), lr=lr)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        act_logit = self.action_layers(x)\n",
    "        values = self.value_layers(x)\n",
    "        return act_logit, values\n",
    "\n",
    "    def forward(self, x):\n",
    "        act_logit, values = self._forward(x)\n",
    "\n",
    "        dist = torch.distributions.categorical.Categorical(logits=act_logit)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, values, log_prob\n",
    "\n",
    "    def evaluate_actions(self, observation, action):\n",
    "        act_logit, values = self._forward(observation)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=act_logit)\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return values, log_prob, entropy\n",
    "\n",
    "\n",
    "def run_experiment(args):\n",
    "\n",
    "    seed = args.seed\n",
    "    if args.seed is None:\n",
    "        seed = np.random.randint(0, 2**16)\n",
    "\n",
    "    # Logger Callbacks\n",
    "    rollout_callback = LogRolloutCallback()\n",
    "    learn_callback = InitLogCallback(args.log_interval,\n",
    "                                     args.log_dir)\n",
    "    weight_callback = LogWeightCallback(\"weights.json\")\n",
    "    grad_callback = LogGradCallback(\"grads.json\")\n",
    "\n",
    "    # Environment\n",
    "    vecenv = make_vec_env(wrap_env,\n",
    "                          n_envs=args.n_envs,\n",
    "                          seed=seed,\n",
    "                          vec_env_cls=SubprocVecEnv)\n",
    "\n",
    "    # Policy\n",
    "    policy = Policy(vecenv.observation_space,\n",
    "                    vecenv.action_space,\n",
    "                    hidden_size=args.hiddensize,\n",
    "                    lr=args.lr)\n",
    "\n",
    "    # Modules\n",
    "    buffer = RolloutBuffer(buffer_size=args.n_steps,\n",
    "                           observation_space=vecenv.observation_space,\n",
    "                           action_space=vecenv.action_space,\n",
    "                           device=args.device,\n",
    "                           gae_lambda=args.gae_lambda,\n",
    "                           gamma=args.gamma,\n",
    "                           n_envs=args.n_envs)\n",
    "    collector = OnPolicyCollector(env=vecenv,\n",
    "                                  buffer=buffer,\n",
    "                                  policy=policy,\n",
    "                                  callbacks=[rollout_callback],\n",
    "                                  device=args.device)\n",
    "    model = A2C(policy=policy,\n",
    "                rollout_buffer=buffer,\n",
    "                rollout_len=args.n_steps,\n",
    "                collector=collector,\n",
    "                env=vecenv,\n",
    "                ent_coef=args.ent_coef,\n",
    "                vf_coef=args.val_coef,\n",
    "                max_grad_norm=args.max_grad_norm,\n",
    "                normalize_advantage=False,\n",
    "                callbacks=[learn_callback, weight_callback, grad_callback],\n",
    "                device=args.device)\n",
    "\n",
    "    # Start learning\n",
    "    model.learn(args.total_timesteps)\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Pong-Ram A2C\")\n",
    "    parser.add_argument(\"--n-envs\", type=int, default=1,\n",
    "                        help=\"Number of parallel environments\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=None,\n",
    "                        help=\"Global seed\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cpu\",\n",
    "                        help=\"Torch device\")\n",
    "    parser.add_argument(\"--hiddensize\", type=int, default=64,\n",
    "                        help=\"Hidden size of the policy\")\n",
    "    parser.add_argument(\"--n-steps\", type=int, default=5,\n",
    "                        help=\"Rollout Length\")\n",
    "    parser.add_argument(\"--batchsize\", type=int, default=32,\n",
    "                        help=\"Batch size of the a2c training\")\n",
    "    parser.add_argument(\"--gae-lambda\", type=float, default=1.0,\n",
    "                        help=\"GAE coefficient\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=7e-4,\n",
    "                        help=\"Learning rate\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.95,  \n",
    "                        help=\"Discount factor\")\n",
    "    parser.add_argument(\"--ent-coef\", type=float, default=0.02,\n",
    "                        help=\"Entropy coefficient\")\n",
    "    parser.add_argument(\"--val-coef\", type=float, default=0.5,\n",
    "                        help=\"Value loss coefficient\")\n",
    "    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n",
    "                        help=\"Maximum allowed graident norm\")\n",
    "    parser.add_argument(\"--total-timesteps\", type=int, default=int(2e4),\n",
    "                        help=(\"Training length interms of cumulative\"\n",
    "                              \" environment timesteps\"))\n",
    "    parser.add_argument(\"--log-interval\", type=int, default=500,\n",
    "                        help=(\"Logging interval in terms of training\"\n",
    "                              \" iterations\"))\n",
    "    parser.add_argument(\"--log-dir\", type=str, default=None,\n",
    "                        help=(\"Logging dir\"))\n",
    "    return parser.parse_args([\n",
    "        \"--log-dir\", \"logs/{}\".format(now),\n",
    "        \"--log-interval\", \"50\",\n",
    "        \"--n-envs\", \"1\",\n",
    "        \"--n-steps\", \"50\",\n",
    "    ])\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "run_experiment(get_args())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizers.visualize import render_layout\n",
    "\n",
    "render_layout(\n",
    "    log_dir=\"logs/{}\".format(now),\n",
    "    layout=[[\"S\", \"S\"], [\"H\", \"H\"]]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
