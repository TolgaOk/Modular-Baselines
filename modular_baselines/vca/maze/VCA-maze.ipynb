{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from modular_baselines.loggers.basic import(InitLogCallback,\n",
    "                                            LogRolloutCallback,\n",
    "                                            LogWeightCallback,\n",
    "                                            LogGradCallback)\n",
    "\n",
    "from modular_baselines.vca.algorithm import DiscerteStateVCA\n",
    "from modular_baselines.vca.buffer import Buffer\n",
    "from modular_baselines.vca.collector import NStepCollector\n",
    "from modular_baselines.vca.modules import (CategoricalPolicyModule,\n",
    "                     CategoricalTransitionModule,\n",
    "                     CategoricalRewardModule)\n",
    "from environment import MazeEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "args = dict(\n",
    "    state_size = 11,\n",
    "    buffer_size = 5000,\n",
    "    policy_hidden_size = 32,\n",
    "    policy_tau = 1,\n",
    "    transition_hidden_size = 32,\n",
    "    transition_module_tau = 1,\n",
    "    reward_set = [-1, 0, 1],\n",
    "    reward_hidden_size = 16,\n",
    "    reward_module_tau = 1,\n",
    "    batchsize = 32,\n",
    "    entropy_coef = 0.001,\n",
    "    rollout_len=5,\n",
    "    total_timesteps=int(2e4),\n",
    "    device=\"cpu\",\n",
    "    log_interval=200,\n",
    "    trans_lr=1e-3,\n",
    "    policy_lr=1e-3,\n",
    "    reward_lr=1e-3,\n",
    "    log_dir=\"logs/{}\".format(now)\n",
    ")\n",
    "args = namedtuple(\"Args\", args.keys())(*args.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to logs/01-01-2021-18-47-56\n"
     ]
    }
   ],
   "source": [
    "env = MazeEnv()\n",
    "vecenv = make_vec_env(lambda: MazeEnv())\n",
    "\n",
    "rollout_callback = LogRolloutCallback()\n",
    "init_callback = InitLogCallback(args.log_interval,\n",
    "                                args.log_dir)\n",
    "weight_callback = LogWeightCallback(\"weights.json\")\n",
    "grad_callback = LogGradCallback(\"grads.json\")\n",
    "\n",
    "buffer = Buffer(\n",
    "    args.buffer_size,\n",
    "    vecenv.observation_space,\n",
    "    vecenv.action_space)\n",
    "\n",
    "policy_m = CategoricalPolicyModule(\n",
    "    vecenv.observation_space.n,\n",
    "    vecenv.action_space.n,\n",
    "    args.policy_hidden_size,\n",
    "    tau=args.policy_tau)\n",
    "trans_m = CategoricalTransitionModule(\n",
    "    vecenv.observation_space.n,\n",
    "    vecenv.action_space.n,\n",
    "    state_set=torch.from_numpy(env.state_set),\n",
    "    hidden_size=args.transition_hidden_size,\n",
    "    tau=args.transition_module_tau)\n",
    "reward_m = CategoricalRewardModule(\n",
    "    vecenv.observation_space.n,\n",
    "    env.reward_set,\n",
    "    args.reward_hidden_size,\n",
    "    tau=args.reward_module_tau)\n",
    "\n",
    "collector = NStepCollector(\n",
    "    env=vecenv,\n",
    "    buffer=buffer,\n",
    "    policy=policy_m,\n",
    "    callbacks=[rollout_callback])\n",
    "algorithm = DiscerteStateVCA(\n",
    "    policy_module=policy_m,\n",
    "    transition_module=trans_m,\n",
    "    reward_module=reward_m,\n",
    "    buffer=buffer,\n",
    "    collector=collector,\n",
    "    env=vecenv,\n",
    "    reward_vals=env.expected_reward(),\n",
    "    rollout_len=args.rollout_len,\n",
    "    trans_opt=torch.optim.RMSprop(trans_m.parameters(), lr=args.trans_lr),\n",
    "    policy_opt=torch.optim.RMSprop(policy_m.parameters(), lr=args.policy_lr),\n",
    "    reward_opt=torch.optim.RMSprop(reward_m.parameters(), lr=args.reward_lr),\n",
    "    batch_size=args.batchsize,\n",
    "    entropy_coef=args.entropy_coef,\n",
    "    device=args.device,\n",
    "    callbacks=[init_callback, weight_callback, grad_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    fps             | 475      |\n",
      "|    iterations      | 0        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 5        |\n",
      "| train/             |          |\n",
      "|    Reward loss     | 0.605    |\n",
      "|    Transition loss | 3.41     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 107      |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 239      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 1005     |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 0.131    |\n",
      "|    Transition loss | 2.74     |\n",
      "|    entropy         | 6.68     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | 0.111    |\n",
      "| time/              |          |\n",
      "|    fps             | 241      |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 2005     |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 0.0202   |\n",
      "|    Transition loss | 1.35     |\n",
      "|    entropy         | 5.7      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 103      |\n",
      "|    ep_rew_mean     | 0.111    |\n",
      "| time/              |          |\n",
      "|    fps             | 236      |\n",
      "|    iterations      | 600      |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 3005     |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 0.00681  |\n",
      "|    Transition loss | 0.678    |\n",
      "|    entropy         | 4.77     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 153      |\n",
      "|    ep_rew_mean     | -0.429   |\n",
      "| time/              |          |\n",
      "|    fps             | 236      |\n",
      "|    iterations      | 800      |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 4005     |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 0.0025   |\n",
      "|    Transition loss | 0.4      |\n",
      "|    entropy         | 4.78     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 164      |\n",
      "|    ep_rew_mean     | -0.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 241      |\n",
      "|    iterations      | 1000     |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 5005     |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 0.000986 |\n",
      "|    Transition loss | 0.252    |\n",
      "|    entropy         | 5.34     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 170      |\n",
      "|    ep_rew_mean     | -0.667   |\n",
      "| time/              |          |\n",
      "|    fps             | 247      |\n",
      "|    iterations      | 1200     |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 6005     |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 0.000239 |\n",
      "|    Transition loss | 0.138    |\n",
      "|    entropy         | 5.7      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 170      |\n",
      "|    ep_rew_mean     | -0.667   |\n",
      "| time/              |          |\n",
      "|    fps             | 252      |\n",
      "|    iterations      | 1400     |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 7005     |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 6.78e-05 |\n",
      "|    Transition loss | 0.0493   |\n",
      "|    entropy         | 5.42     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 157      |\n",
      "|    ep_rew_mean     | -0.333   |\n",
      "| time/              |          |\n",
      "|    fps             | 257      |\n",
      "|    iterations      | 1600     |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 8005     |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 2.98e-05 |\n",
      "|    Transition loss | 0.0368   |\n",
      "|    entropy         | 5.74     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 146      |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 251      |\n",
      "|    iterations      | 1800     |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 9005     |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 1.24e-05 |\n",
      "|    Transition loss | 0.0556   |\n",
      "|    entropy         | 6.56     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 189      |\n",
      "|    ep_rew_mean     | -0.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 238      |\n",
      "|    iterations      | 2000     |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 10005    |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 9.1e-06  |\n",
      "|    Transition loss | 0.0923   |\n",
      "|    entropy         | 6.86     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 239      |\n",
      "|    iterations      | 2200     |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 11005    |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 2.49e-06 |\n",
      "|    Transition loss | 0.0786   |\n",
      "|    entropy         | 6.87     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 185      |\n",
      "|    ep_rew_mean     | -0.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 243      |\n",
      "|    iterations      | 2400     |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 12005    |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 1.01e-06 |\n",
      "|    Transition loss | 0.0605   |\n",
      "|    entropy         | 6.9      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 193      |\n",
      "|    ep_rew_mean     | -0.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 246      |\n",
      "|    iterations      | 2600     |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 13005    |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 4.52e-07 |\n",
      "|    Transition loss | 0.0524   |\n",
      "|    entropy         | 6.91     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 194      |\n",
      "|    ep_rew_mean     | -0.667   |\n",
      "| time/              |          |\n",
      "|    fps             | 249      |\n",
      "|    iterations      | 2800     |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 14005    |\n",
      "| train/             |          |\n",
      "|    E[R]            | 0        |\n",
      "|    Reward loss     | 9.52e-08 |\n",
      "|    Transition loss | 0.0319   |\n",
      "|    entropy         | 6.93     |\n",
      "---------------------------------\n",
      "[[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-865a8ebd646e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Research/modular-baselines/modular_baselines/algorithms/algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/modular-baselines/modular_baselines/vca/collector.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self, n_rollout_steps)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvalues_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stable-baselines3/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/stable-baselines3/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             )\n",
      "\u001b[0;32m~/Research/stable-baselines3/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/modular-baselines/modular_baselines/vca/maze/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Research/modular-baselines/modular_baselines/vca/maze/environment.py\u001b[0m in \u001b[0;36mprocess_observation\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"$!\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!$!"
     ]
    }
   ],
   "source": [
    "algorithm.learn(args.total_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6954fd0d309e4bba84ce85e5c430a9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(VBox(children=(Dropdown(description='X axis', options=('time/total_timesteps', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from visualizers.visualize import render_layout\n",
    "\n",
    "render_layout(\n",
    "    log_dir=\"logs/{}\".format(now),\n",
    "    layout=[[\"S\", \"S\"], [\"H\", \"H\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
